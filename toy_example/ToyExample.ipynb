{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import time\n",
    "from data_import import *\n",
    "from graph_manipulation import *\n",
    "from time_aware_splits import *\n",
    "from popularity_based_rec import *\n",
    "from personalized_prank import *\n",
    "from pathsim import *\n",
    "from simrank import *\n",
    "from accuracy_evaluation import *\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "from background_jobs import create_graph\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import plotly.graph_objs as go\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import datetime\n",
    "import itertools\n",
    "import copy\n",
    "import sys\n",
    "from time import sleep\n",
    "from collections import defaultdict\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from pyvis.network import Network as pyvisNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "pyvis_network = pyvisNet(notebook=True,width = 1000)\n",
    "patients = ['Patient1', 'Patient2', 'Patient3']\n",
    "node_color = []\n",
    "for patient in patients:\n",
    "    node_color.append(\"#00ff1e\")\n",
    "pyvis_network.add_nodes(patients,label=patients,color=node_color)\n",
    "\n",
    "drugs = ['Drug1', 'Drug2', 'Drug3', 'Drug4','Drug5','Drug6','Drug7']\n",
    "node_color = []\n",
    "for drug in drugs:\n",
    "    node_color.append(\"#162347\")\n",
    "pyvis_network.add_nodes(drugs,label=drugs, color = node_color)\n",
    "treatments = ['T'+str(i) for i in range(1,7)]\n",
    "node_color = []\n",
    "for treatment in treatments:\n",
    "    node_color.append(\"#dd4b39\")\n",
    "pyvis_network.add_nodes(treatments,label=treatments,color = node_color)\n",
    "G.add_nodes_from(patients, entity='P')\n",
    "G.add_nodes_from(drugs, entity='D')\n",
    "for treatment in treatments[:3]:\n",
    "    treatment_start = pd.to_datetime('2016-06-30 09:30:00')\n",
    "    G.add_node(treatment,datetime = treatment_start, entity='T')\n",
    "for treatment in treatments[3:]:\n",
    "    treatment_start = pd.to_datetime('2017-01-30 09:30:00')\n",
    "    G.add_node(treatment,datetime = treatment_start, entity='T')\n",
    "G.add_node('T8',datetime= pd.to_datetime('2016-01-30 09:30:00'),entity = 'T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Patients - Treatments\n",
    "for i in (0,3):\n",
    "    G.add_edge('Patient1', treatments[i], type='PT')\n",
    "    pyvis_network.add_edge('Patient1', treatments[i])\n",
    "for i in (1,4):\n",
    "    G.add_edge('Patient2', treatments[i], type='PT')\n",
    "    pyvis_network.add_edge('Patient2', treatments[i])\n",
    "for i in (2,5):\n",
    "    G.add_edge('Patient3', treatments[i], type='PT')\n",
    "    pyvis_network.add_edge('Patient3', treatments[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Treatments - Drugs\n",
    "for i in [0]:\n",
    "    G.add_edge(treatments[i], 'Drug1', type='TD',reading_datetime = pd.to_datetime('2016-06-30 09:30:00'))\n",
    "    G.add_edge(treatments[i], 'Drug2', type='TD',reading_datetime = pd.to_datetime('2016-06-30 09:31:00'))\n",
    "    G.add_edge(treatments[i], 'Drug3', type='TD',reading_datetime = pd.to_datetime('2016-06-30 09:32:00'))\n",
    "    G.add_edge(treatments[i], 'Drug4', type='TD', reading_datetime=pd.to_datetime('2016-06-30 09:33:00'))\n",
    "    pyvis_network.add_edge(treatments[i], 'Drug1')\n",
    "    pyvis_network.add_edge(treatments[i], 'Drug2')\n",
    "    pyvis_network.add_edge(treatments[i], 'Drug3')\n",
    "    pyvis_network.add_edge(treatments[i], 'Drug4')\n",
    "for i in [1]:\n",
    "    G.add_edge(treatments[i], 'Drug1', type='TD',reading_datetime = pd.to_datetime('2016-06-30 09:30:00'))\n",
    "    G.add_edge(treatments[i], 'Drug2', type='TD',reading_datetime = pd.to_datetime('2016-06-30 09:31:00'))\n",
    "    G.add_edge(treatments[i], 'Drug3', type='TD',reading_datetime = pd.to_datetime('2016-06-30 09:33:00'))\n",
    "    G.add_edge(treatments[i], 'Drug4', type='TD',reading_datetime = pd.to_datetime('2016-06-30 09:32:00'))\n",
    "    G.add_edge(treatments[i], 'Drug5', type='TD', reading_datetime=pd.to_datetime('2016-06-30 09:34:00'))\n",
    "    G.add_edge(treatments[i], 'Drug7', type='TD', reading_datetime=pd.to_datetime('2016-06-30 09:35:00'))\n",
    "    pyvis_network.add_edge(treatments[i], 'Drug1')\n",
    "    pyvis_network.add_edge(treatments[i], 'Drug2')\n",
    "    pyvis_network.add_edge(treatments[i], 'Drug3')\n",
    "    pyvis_network.add_edge(treatments[i], 'Drug4')\n",
    "    pyvis_network.add_edge(treatments[i], 'Drug5')\n",
    "    pyvis_network.add_edge(treatments[i], 'Drug7')\n",
    "for i in [2]:\n",
    "    G.add_edge(treatments[i], 'Drug1', type='TD',reading_datetime = pd.to_datetime('2016-06-30 09:30:00'))\n",
    "    G.add_edge(treatments[i], 'Drug2', type='TD',reading_datetime = pd.to_datetime('2016-06-30 09:31:00'))\n",
    "    G.add_edge(treatments[i], 'Drug5', type='TD',reading_datetime = pd.to_datetime('2016-06-30 09:32:00'))\n",
    "    G.add_edge(treatments[i], 'Drug6', type='TD', reading_datetime=pd.to_datetime('2016-06-30 09:33:00'))\n",
    "    pyvis_network.add_edge(treatments[i], 'Drug1')\n",
    "    pyvis_network.add_edge(treatments[i], 'Drug2')\n",
    "    pyvis_network.add_edge(treatments[i], 'Drug5')\n",
    "    pyvis_network.add_edge(treatments[i], 'Drug6')\n",
    "\n",
    "for i in [3]:\n",
    "    G.add_edge(treatments[i], 'Drug1', type='TD',reading_datetime = pd.to_datetime('2017-01-30 09:30:00'))\n",
    "    G.add_edge(treatments[i], 'Drug2', type='TD',reading_datetime = pd.to_datetime('2017-01-30 09:31:00'))\n",
    "    G.add_edge(treatments[i], 'Drug3', type='TD',reading_datetime = pd.to_datetime('2017-01-30 09:32:00'))\n",
    "    pyvis_network.add_edge(treatments[i], 'Drug1')\n",
    "    pyvis_network.add_edge(treatments[i], 'Drug2')\n",
    "    pyvis_network.add_edge(treatments[i], 'Drug3')\n",
    "\n",
    "for i in [4]:\n",
    "    G.add_edge(treatments[i], 'Drug1', type='TD',reading_datetime = pd.to_datetime('2017-01-30 09:30:00'))\n",
    "    G.add_edge(treatments[i], 'Drug2', type='TD',reading_datetime = pd.to_datetime('2017-01-30 09:31:00'))\n",
    "    G.add_edge(treatments[i], 'Drug3', type='TD',reading_datetime = pd.to_datetime('2017-01-30 09:33:00'))\n",
    "    G.add_edge(treatments[i], 'Drug4', type='TD',reading_datetime = pd.to_datetime('2017-01-30 09:32:00'))\n",
    "    G.add_edge(treatments[i], 'Drug5', type='TD', reading_datetime=pd.to_datetime('2017-01-30 09:35:00'))\n",
    "    pyvis_network.add_edge(treatments[i], 'Drug1')\n",
    "    pyvis_network.add_edge(treatments[i], 'Drug2')\n",
    "    pyvis_network.add_edge(treatments[i], 'Drug3')\n",
    "    pyvis_network.add_edge(treatments[i], 'Drug4')\n",
    "    pyvis_network.add_edge(treatments[i], 'Drug5')\n",
    "\n",
    "for i in [5]:\n",
    "    G.add_edge(treatments[i], 'Drug1', type='TD',reading_datetime = pd.to_datetime('2017-01-30 09:30:00'))\n",
    "    G.add_edge(treatments[i], 'Drug2', type='TD',reading_datetime = pd.to_datetime('2017-01-30 09:31:00'))\n",
    "    G.add_edge(treatments[i], 'Drug5', type='TD',reading_datetime = pd.to_datetime('2017-01-30 09:32:00'))\n",
    "    G.add_edge(treatments[i], 'Drug3', type='TD',reading_datetime = pd.to_datetime('2017-01-30 09:33:00'))\n",
    "    pyvis_network.add_edge(treatments[i], 'Drug1')\n",
    "    pyvis_network.add_edge(treatments[i], 'Drug2')\n",
    "    pyvis_network.add_edge(treatments[i], 'Drug5')\n",
    "    pyvis_network.add_edge(treatments[i], 'Drug3')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_example = True\n",
    "dataset = 'medical' #posssible values : Adressa,german_news, italian_news, german_tvbroadcasts, medical\n",
    "edge_type = 'edge_type'\n",
    "if toy_example:\n",
    "    custom_user = 'P'\n",
    "    custom_session = 'T'\n",
    "    custom_article = 'D'\n",
    "    edge_type = 'type'\n",
    "else:\n",
    "    G = nx.read_gpickle(f'./Data/{dataset}.gpickle')#Comment this out if you want to run the toy example\n",
    "if dataset =='medical':\n",
    "    custom_user = 'P'\n",
    "    custom_session = 'T'\n",
    "    custom_article = 'D'\n",
    "else:\n",
    "    custom_user = 'U'\n",
    "    custom_session = 'S'\n",
    "    custom_article = 'A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show nodes and edges of Graph\n",
    "entities = []\n",
    "for entity in G._node:\n",
    "    if G._node[entity]['entity'] in entities:\n",
    "        1\n",
    "    else:\n",
    "        entities.append(G._node[entity]['entity'])\n",
    "pos = nx.spring_layout(G)\n",
    "color_dict = {}\n",
    "edges_list = []\n",
    "for i in range(0,len(entities)):\n",
    "    color_dict[list(entities)[i]] = 1/len(entities) * i\n",
    "values = [color_dict.get(node[0],0.25) for node in G.nodes()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P', 'D', 'T']\n"
     ]
    }
   ],
   "source": [
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyvis_g = pyvisNet(notebook=True,width=1000)\n",
    "pyvis_g.from_nx(G)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"500px\"\n",
       "            src=\"mygraph.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fc6a104ac10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyvis_network.show(\"mygraph.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Graph Size for the remainder of notebook\n",
    "plt.clf()\n",
    "fig_size = (12,6)\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_metapaths(subgraphs):\n",
    "    metapaths=[]\n",
    "    def metapath_length_two(subgraph):\n",
    "        temp_metapath_list = []\n",
    "        for i in range(0,len(subgraph)):\n",
    "            start_node = subgraph[i]\n",
    "            end_node = subgraph[1-i]\n",
    "            metapath_name = f'{start_node}_{end_node}_{start_node}'\n",
    "            metapath_length = 1\n",
    "            if metapath_name in metapaths:\n",
    "                1\n",
    "            else:\n",
    "                metapaths.append(metapath_name)\n",
    "                temp_metapath_list.append(dict(start_node=start_node,end_node=end_node,metapath_name=metapath_name,length=metapath_length,subgraph=subgraph))\n",
    "        return temp_metapath_list\n",
    "    def metapath_length_three(subgraph):\n",
    "        temp_metapath_list = []\n",
    "        for i in [0,2]:\n",
    "            start_node = subgraph[i]\n",
    "            temp_node = subgraph[1]\n",
    "            end_node = subgraph[2 - i]\n",
    "            metapath_name = f'{start_node}_{temp_node}_{end_node}_{temp_node}_{start_node}'\n",
    "            metapath_length = 2\n",
    "            temp_metapath_list.append(\n",
    "                dict(start_node=start_node, end_node=end_node, metapath_name=metapath_name, length=metapath_length,subgraph=subgraph))\n",
    "        return temp_metapath_list\n",
    "    metapath_dict = []\n",
    "    for subgraph in subgraphs:\n",
    "        if len(subgraph)==2:\n",
    "            metapath_dict.extend(metapath_length_two(subgraph))\n",
    "        elif len(subgraph)==3:\n",
    "            metapath_dict.extend(metapath_length_three(subgraph))\n",
    "    return metapath_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gm = GraphManipulation()\n",
    "gm.G = G\n",
    "patients = [n for n, attr in G.nodes(data=True) if attr['entity'] == custom_user]\n",
    "print(f' Number of patients {len(patients)}')\n",
    "methods = ['PathCount']#PathCount,Pathsim,RWR,POP,Simrank\n",
    "\n",
    "nodes = [custom_user,custom_session,custom_article,'C']\n",
    "min_items_n = 3\n",
    "number_splits = 12\n",
    "short_days = 2\n",
    "print_output = 1\n",
    "number_recommendations = 3\n",
    "# print(methods)\n",
    "possible_subgraphs = [(custom_user,custom_article),(custom_session,custom_article),(custom_user,custom_session),\n",
    "                      (custom_user,custom_session,custom_article),(custom_user,custom_session,custom_article,'C')]\n",
    "weighted_strategy_method = 1 #options are 1 - normal mean,\n",
    "                             #2 - weighted mean (sigmoid)\n",
    "possible_metapaths=[]\n",
    "explainability = defaultdict(list)\n",
    "subgraphs =[]\n",
    "for length in range(2,len(nodes)+1):\n",
    "    x = list(itertools.permutations(nodes,length))\n",
    "    for item in x:\n",
    "        if item in possible_subgraphs:\n",
    "            subgraphs.append(item)\n",
    "possible_metapaths = []\n",
    "possible_metapaths.extend(get_possible_metapaths(subgraphs))\n",
    "# print(subgraphs)\n",
    "# gm.filter_users(gm.G, n_sessions=min)\n",
    "gm.G = G\n",
    "gm.filter_sessions(gm.G, n_items=min_items_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = \"\"\n",
    "e+=(f'\\nGENERAL STATISTICS')\n",
    "e+=(f'\\nNumber of patients:{len(gm.get_nodes(G,custom_user))}')\n",
    "e +=(f'\\nNumber of treatments:{len(gm.get_nodes(G,custom_session))}')\n",
    "e +=(f'\\nNumber of drugs:{len(gm.get_nodes(G,custom_article))}')\n",
    "\n",
    "\n",
    "art_per_session = gm.get_articles_per_session(gm.G,custom_session)\n",
    "e +=(f'\\nAvg number of drugs per session:{round(np.mean(art_per_session), 2)}')\n",
    "e +=(f'\\nMax number of drugs per session:{round(np.max(art_per_session), 2)}')\n",
    "\n",
    "ses_per_user = gm.get_sessions_per_user(gm.G,custom_user)\n",
    "e +=(f'\\nAvg number of treatments per patient:{round(np.mean(ses_per_user), 2)}')\n",
    "e +=(f'\\nMax number of treatments per patient:{round(np.max(ses_per_user), 2)} ')\n",
    "print (e)\n",
    "#Create time splits based on treatments\n",
    "tas = TimeAwareSplits(G,session_entity_prefix= custom_session)\n",
    "tas.create_time_split_graphs(G, num_splits=number_splits)\n",
    "tas.create_time_window_graphs(short_days)\n",
    "short_back_timedelta = datetime.timedelta(days=short_days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tas.time_window_graph_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "_dump_process = True\n",
    "\n",
    "e +=(f'\\n\\nTime span list:\\n')\n",
    "counter = 0\n",
    "for timespan in tas.time_span_list:\n",
    "\n",
    "    e+=(f'{timespan}\\n')\n",
    "    counter+=1\n",
    "    if counter>1:\n",
    "        counter=0\n",
    "ae = AccuracyEvaluation(G,number_recommendations=number_recommendations,edge_type = edge_type)\n",
    "\n",
    "train_set_len = []\n",
    "train_len_dict = defaultdict(list)\n",
    "n_articles_train = []\n",
    "n_recommendation = dict()\n",
    "sessions_per_user_in_short_term = []\n",
    "avg_ses_len = defaultdict(list)\n",
    "train_set_len = []\n",
    "train_len_dict = defaultdict(list)\n",
    "n_articles_train = []\n",
    "n_recommendation = dict()\n",
    "sessions_per_user_in_short_term = []\n",
    "avg_ses_len = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for tw_i, tw_iter in enumerate(tas.time_window_graph_list):\n",
    "    print(f'\\n\\n======= Time split{tw_i} =======')\n",
    "            \n",
    "    n_recommendation[tw_i] = 0\n",
    "    n_recommendation[f'{tw_i}_correct'] = 0\n",
    "    # long_train_g = tw_iter[0]\n",
    "    tw_iter[1].frozen = False\n",
    "    test_g = tw_iter[1].copy()\n",
    "    \n",
    "    test_treatments = sorted(\n",
    "        [(s, attr['datetime']) for s, attr in test_g.nodes(data=True) if attr['entity'] == custom_session],\n",
    "        key=lambda x: x[1])\n",
    "    for (s, s_datetime) in test_treatments:\n",
    "        #         print([n for n in nx.neighbors(test_g,s)])\n",
    "        #         print(test_g.get_edge_data(s,'P1'))\n",
    "        user = [n for n in nx.neighbors(test_g, s) if test_g.get_edge_data(s, n)[edge_type] == f'{custom_user}{custom_session}'][0]\n",
    "        test_session_G = nx.Graph()\n",
    "        test_session_G.add_node(user, entity=custom_user)\n",
    "        test_session_G.add_node(s, entity=custom_session)\n",
    "        test_session_G.add_edge(user, s, edge_type=f'{custom_user}{custom_session}')\n",
    "        drugs = sorted([n for n in nx.neighbors(test_g, s) if test_g.get_edge_data(s, n)[edge_type] == f'{custom_session}{custom_article}'],\n",
    "                       key=lambda x: test_g.get_edge_data(s, x)['reading_datetime'])\n",
    "        \n",
    "        print(f'\\nPatient : {user}')\n",
    "        print(f'\\nDrugs {drugs} of treatment {s} for patient {user}\\n')\n",
    "        \n",
    "        for i in range(min_items_n, len(drugs)):\n",
    "            e=''\n",
    "            methods_to_be_evaluated = []\n",
    "            methos_to_be_evaluated_explainable = []\n",
    "            # short_train_g = tas.create_short_term_train_set(s_datetime, short_back_timedelta,\n",
    "            #                                                 test_session_graph=test_session_G)\n",
    "            train_start = s_datetime - short_back_timedelta\n",
    "            train_end = s_datetime\n",
    "\n",
    "            temp_sessions = [n for n, attr in G.nodes(data=True)\n",
    "                             if attr['entity'] == custom_session\n",
    "                             and attr['datetime'] >= train_start\n",
    "                             and attr['datetime'] <= train_end]\n",
    "\n",
    "            # temp_neighbors = []\n",
    "            # for s in temp_sessions:\n",
    "            #     temp_neighbors.extend(nx.neighbors(self.G, s))\n",
    "            #\n",
    "            # temp_neighbors = list(set(temp_neighbors))\n",
    "            #\n",
    "            # categories = [n for n, attr in self.G.nodes(data=True) if attr['entity'] == 'C']\n",
    "\n",
    "            temp_users = []\n",
    "            temp_articles = []\n",
    "            for session_test in temp_sessions:\n",
    "                temp_users.extend([u for u in G[session_test] if G[session_test][u][edge_type] == f'{custom_user}{custom_session}'])\n",
    "                temp_articles.extend([a for a in G[session_test] if G[session_test][a][edge_type] == f'{custom_session}{custom_article}'])\n",
    "\n",
    "            temp_users = list(set(temp_users))\n",
    "            temp_articles = list(set(temp_articles))\n",
    "\n",
    "            temp_categories = []\n",
    "            for a in temp_articles:\n",
    "                temp_categories.extend([c for c in G[a] if G[a][c][edge_type] == f'{custom_article}C'])\n",
    "\n",
    "            temp_categories = list(set(temp_categories))\n",
    "\n",
    "            temp_locations = []\n",
    "            for a in temp_articles:\n",
    "                temp_locations.extend([l for l in G[a] if G[a][l][edge_type] == f'{custom_article}E'])\n",
    "\n",
    "                temp_locations = list(set(temp_locations))\n",
    "\n",
    "            temp_nodes = []\n",
    "            temp_nodes.extend(temp_sessions)\n",
    "            # temp_nodes.extend(temp_neighbors)\n",
    "            # temp_nodes.extend(categories)\n",
    "            temp_nodes.extend(temp_users)\n",
    "            temp_nodes.extend(temp_articles)\n",
    "            temp_nodes.extend(temp_categories)\n",
    "            temp_nodes.extend(temp_locations)\n",
    "\n",
    "            short_train_subgraph = G.subgraph(temp_nodes)\n",
    "\n",
    "\n",
    "            short_train_subgraph = nx.compose(short_train_subgraph, test_session_G)\n",
    "\n",
    "            short_train_g =  short_train_subgraph\n",
    "            if len(short_train_g) == 0:\n",
    "                continue\n",
    "            test_session_G.add_nodes_from(drugs[:i], entity=custom_article)\n",
    "            for d in drugs[:i]:\n",
    "                test_session_G.add_edge(s, d, edge_type=f'{custom_session}{custom_article}')\n",
    "            # ptd_train = gm.create_subgraph_of_adjacent_entities(short_train_g,list_of_entities=['P','T','D'])\n",
    "            subgraphs_train = []\n",
    "            subgraph_dict = {}\n",
    "            for subgraph in subgraphs:\n",
    "                if len(subgraph) == 2:\n",
    "                    if subgraph==(custom_user, custom_article):\n",
    "                        path_len = 2\n",
    "                    else:\n",
    "                        path_len = 1\n",
    "\n",
    "                    derived_graph =gm.derive_adjacency_multigraph(short_train_g,entity1=subgraph[0][0],entity2=subgraph[1],path_len=path_len)\n",
    "                    subgraph_dict[subgraph] = derived_graph\n",
    "                    subgraphs_train.append((derived_graph,\n",
    "                                            f'{subgraph[0]}_{subgraph[1]}',subgraph))\n",
    "                elif len(subgraph) == 3:\n",
    "\n",
    "\n",
    "                    derived_graph = gm.create_subgraph_of_adjacent_entities(short_train_g,\n",
    "                                                            list_of_entities=[\n",
    "                                                                subgraph[0],\n",
    "                                                                subgraph[1], subgraph[2]])\n",
    "                    subgraph_dict[subgraph] = derived_graph\n",
    "                    subgraphs_train.append((derived_graph,\n",
    "                                            f'{subgraph[0]}_{subgraph[1]}_{subgraph[2]}',subgraph))\n",
    "                elif len(subgraph) == 4:\n",
    "\n",
    "                    derived_graph = gm.create_subgraph_of_adjacent_entities(short_train_g,\n",
    "                                                                                    list_of_entities=[\n",
    "                                                                                        subgraph[0],\n",
    "                                                                                        subgraph[1],\n",
    "                                                                                        subgraph[2], subgraph[3]])\n",
    "                    subgraph_dict[subgraph] = derived_graph\n",
    "                    subgraphs_train.append((derived_graph,\n",
    "                                            f'{subgraph[0]}_{subgraph[1]}_{subgraph[2]}_{subgraph[3]}',subgraph))\n",
    "                else:\n",
    "\n",
    "                    derived_graph = gm.create_subgraph_of_adjacent_entities(short_train_g,\n",
    "                                                                                    list_of_entities=[\n",
    "                                                                                        subgraph[0],\n",
    "                                                                                        subgraph[1],\n",
    "                                                                                        subgraph[2],\n",
    "                                                                                        subgraph[3], subgraph[4]])\n",
    "                    subgraph_dict[subgraph] = derived_graph\n",
    "                    subgraphs_train.append((derived_graph,\n",
    "                                            f'{subgraph[0]}_{subgraph[1]}_{subgraph[2]}_{subgraph[3]}_{subgraph[4]}',subgraph))\n",
    "            \n",
    "            if 'POP' in methods:\n",
    "                pop = PopularityBasedRec(G, number_recommendations)\n",
    "                pop.compute_pop(short_train_g)\n",
    "                pop_rec = pop.predict_next(user, drugs[:i])\n",
    "\n",
    "                #                         if len(pop_rec) == 0:\n",
    "                #                             continue\n",
    "                #                         else:\n",
    "                methods_to_be_evaluated.append((pop_rec, 'POP'))\n",
    "\n",
    "                # ------- SimRank ----------------------\n",
    "            if 'Simrank' in methods:\n",
    "                simrank_models = []\n",
    "                for subgraph_train in subgraphs_train:\n",
    "                    simrank = SimRankRec(number_recommendations,custom_user,custom_session,custom_article)\n",
    "                    simrank.compute_similarity_matrix(subgraph_train[0], max_iter=10,result_type=custom_article)\n",
    "                    simrank_models.append((simrank, subgraph_train[1]))\n",
    "                for simrank_model in simrank_models:\n",
    "                    recommendation = simrank_model[0].predict_next(user, drugs[:i], method=weighted_strategy_method,)\n",
    "                    #                             if len(recommendation) == 0:\n",
    "                    #                                 continue\n",
    "                    #                             else:\n",
    "#                     print(f'Recommendation : {recommendation}')\n",
    "                    methods_to_be_evaluated.append((recommendation, f'Simrank_{simrank_model[1]}'))\n",
    "\n",
    "                    # ------- RWR --------------------------\n",
    "            if 'RWR' in methods:\n",
    "                rwr_models = []\n",
    "                for subgraph_train in subgraphs_train:\n",
    "                    RWR = PersonalizedPageRankBasedRec(number_recommendations,custom_user,custom_session,custom_article)\n",
    "                    RWR.compute_transition_matrix(subgraph_train[0])\n",
    "                    RWR.create_itemitem_matrix()\n",
    "                    rwr_models.append((RWR, subgraph_train[1]))\n",
    "                for rwr_model in rwr_models:\n",
    "                    recommendation = rwr_model[0].predict_next(user, drugs[:i],method=weighted_strategy_method)\n",
    "                    #                             if len(recommendation) == 0:\n",
    "                    #                                 continue\n",
    "                    #                             else:\n",
    "                    methods_to_be_evaluated.append((recommendation, f'RWR_{rwr_model[1]}'))\n",
    "            if 'Pathsim' in methods:\n",
    "                pathsim_models = []\n",
    "                #for subgraph_train in subgraphs_train:\n",
    "\n",
    "                #nodes = subgraph_train[1].split('_')\n",
    "                for metapath in possible_metapaths:\n",
    "                        pathsim = PathSimRec(number_recommendations)\n",
    "                            #if f'A_{node}_A' not in metapaths_a:\n",
    "                        start_node = metapath['start_node']\n",
    "                        end_node = metapath['end_node']\n",
    "                        metapath_name= metapath['metapath_name']\n",
    "                        length = metapath['length']\n",
    "                        pathsim.compute_similarity_matrix(subgraph_dict[metapath['subgraph']], start_node, end_node, length)\n",
    "                        #pathsim_models.append((pathsim, f'Pathsim_{metapath_name}'))\n",
    "                        #metapaths_a.append(f'A_{node}_A')\n",
    "                    #if ('U' in nodes) and ('S' in nodes) and ('A' in nodes):\n",
    "                        # pathsim.compute_similarity_matrix(short_train_g, 'A', 'U', 2)\n",
    "                        # pathsim_models.append((pathsim, f'Pathsim_M_S_U_S_M'))\n",
    "                # for pathsim_model in pathsim_models:\n",
    "\n",
    "                        if metapath['start_node']==custom_user:\n",
    "                            similar_users = pathsim.get_similar_users(user, gm.get_users(subgraph_dict[metapath['subgraph']]))\n",
    "                            recommendation = pathsim.predict_next_by_UB(similar_users, drugs[:i], subgraph_dict[metapath['subgraph']], topN=True)\n",
    "                        elif metapath['start_node']==custom_session:\n",
    "                            recommendation = pathsim.predict_next_by_SB(s, treatments[:i], topN=True,type=f'{custom_session}{custom_article}')\n",
    "                        elif metapath['start_node'] == custom_article:\n",
    "                            recommendation = pathsim.predict_next_by_AB(drugs[:i], option='sb', topN=True)\n",
    "                        # recommendation = pathsim.predict_next(user, drugs[:i], method=2)\n",
    "                        #                                 if len(recommendation)==0:\n",
    "                        #                                     continue\n",
    "                        #                                 else:\n",
    "                        methods_to_be_evaluated.append((recommendation, f'Pathsim_{metapath_name}'))\n",
    "            \n",
    "            if 'PathCount' in methods:\n",
    "                pathcounts = []\n",
    "                pathcount_models = []\n",
    "                ab_rec = []\n",
    "                sb_rec = []\n",
    "\n",
    "                # if 'A' in nodes:\n",
    "                #     # for path_len in range(1,len(nodes)-1):\n",
    "                #     for node in nodes:\n",
    "                #         if node != 'A':\n",
    "                #             if f'A_{node}_A' not in pathcounts:\n",
    "                \n",
    "                for metapath in possible_metapaths:\n",
    "                    start_node = metapath['start_node']\n",
    "                    end_node = metapath['end_node']\n",
    "                    metapath_name = metapath['metapath_name']\n",
    "                    pathcounts.append(metapath_name)\n",
    "                    length = metapath['length']\n",
    "                    pathsim = PathSimRec(number_recommendations)\n",
    "                    pathsim.compute_similarity_matrix(subgraph_dict[metapath['subgraph']], start_node, end_node, length)\n",
    "                    \n",
    "                    if metapath['start_node'] == custom_user:\n",
    "                        similar_users = pathsim.get_similar_users(user,\n",
    "                                                                  gm.get_users(subgraph_dict[metapath['subgraph']]))\n",
    "                        pathcount_rec_dict = pathsim.predict_next_by_UB(similar_users, drugs[:i],\n",
    "                                                                    subgraph_dict[metapath['subgraph']], topN=True)\n",
    "                    elif metapath['start_node'] == custom_session:\n",
    "                        pathcount_rec_dict= pathsim.predict_next_by_SB(s, treatments[:i], topN=True,type=f'{custom_session}{custom_article}')\n",
    "                    elif metapath['start_node'] == custom_article:\n",
    "                        pathcount_rec_dict= pathsim.predict_next_by_AB(drugs[:i], option='sb', topN=True)\n",
    "                    # recommendation = pathsim.predict_next(user, drugs[:i], method=2)\n",
    "\n",
    "                    # pathcount_rec_dict =pathsim.predict_next_by_AB(drugs[:i], option='ib',topN=False)\n",
    "                    pathcount_models.append((pathsim,f'PathCount_{metapath_name}',list(pathcount_rec_dict.keys())[:number_recommendations],pathcount_rec_dict,pathsim.get_avg_n_of_connected_sessions()))\n",
    "                    ab_rec.append(list(pathcount_rec_dict.keys())[:number_recommendations])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                     #Combine Recs\n",
    "                rec_ab_df = pd.DataFrame(index=set(x for l in ab_rec for x in l), columns=pathcounts)\n",
    "                for a in rec_ab_df.index:\n",
    "                    for pathcount in pathcount_models:\n",
    "                        if len(pathcount[2])>0:\n",
    "\n",
    "                                rec_ab_df.loc[a, f'{pathcount[1][10:]}'] = pathcount[3][a] if a in list(pathcount[3].keys()) else 0\n",
    "                rec_ab_df = rec_ab_df.dropna(axis=1)\n",
    "                if f'{custom_session}_{custom_article}_{custom_session}' in rec_ab_df.columns:\n",
    "                    rec_importance_df = rec_ab_df[f'{custom_session}_{custom_article}_{custom_session}']\n",
    "                    for a in rec_importance_df.index:\n",
    "                        for pathcount in pathcount_models:\n",
    "                            if len(pathcount[2])>0:\n",
    "                                if f'{pathcount[1][10:]}' ==f'{custom_session}_{custom_article}_{custom_session}':\n",
    "                                    rec_importance_df.loc[a,] = round(rec_importance_df.loc[a] /\n",
    "                                                                 pathcount[0].get_avg_n_of_connected_sessions(), 2)\n",
    "                        # rec_importance_df.loc[a, 'SASA'] = round(rec_importance_df.loc[a, 'SASA'] /\n",
    "                        #                                          PathSim_SAS.get_avg_n_of_connected_sessions(), 2)\n",
    "                        # rec_importance_df.loc[a, 'SCSA'] = round(rec_importance_df.loc[a, 'SCSA'] /\n",
    "                        #                                          PathSim_SCS.get_avg_n_of_connected_sessions(), 2)\n",
    "                        # rec_importance_df.loc[a, 'SLSA'] = round(rec_importance_df.loc[a, 'SLSA'] /\n",
    "                        #                                          PathSim_SLS.get_avg_n_of_connected_sessions(), 2)\n",
    "                    # rec_importance_df['vote_sum'] = rec_importance_df['T_D_T']\n",
    "                    explainability[s] = dict(zip(rec_importance_df.index, rec_importance_df))\n",
    "#                     print(f'Explainability {explainability}')\n",
    "                    ae.explainability = explainability\n",
    "                for pathcount in pathcount_models:\n",
    "                        recommendation = pathcount[3]\n",
    "                        # if len(recommendation) == 0:\n",
    "                        #     continue\n",
    "                        # else:\n",
    "                        methos_to_be_evaluated_explainable.append((recommendation, f'{pathcount[1]}',rec_ab_df))\n",
    "                \n",
    "                rec_ab_df['sum'] = rec_ab_df.sum(axis=1)\n",
    "                rec_ab_df = rec_ab_df.sort_values(by='sum', ascending=False)\n",
    "                rec_ab_df = rec_ab_df.drop('sum',axis=1)\n",
    "#                 ax = rec_ab_df.rank().plot(kind='barh', stacked=True, title=f'{user}-{drugs[i]}')\n",
    "                methos_to_be_evaluated_explainable.append((dict(rec_ab_df.sum(axis=1).sort_values(ascending=False)[:number_recommendations]),'PathCount_combined',rec_ab_df))\n",
    "                chart_values = np.asarray(rec_ab_df.values)\n",
    "#                 totals = []\n",
    "\n",
    "#                 for ix in ax.patches:\n",
    "#                     totals.append(ix.get_width())\n",
    "# #                 print(totals)\n",
    "# #                 print(chart_values)\n",
    "#                 for ix in ax.patches:\n",
    "#                     # get_width pulls left or right; get_y pushes up or down\n",
    "#                     ax.text(ix.get_x() + (ix.get_width()/2), ix.get_y() + .5,str(round((ix.get_width()), 2)) , fontsize=15, color='white')\n",
    "#                 plt.show()\n",
    "\n",
    "\n",
    "            n_recommendation[tw_i] += number_recommendations\n",
    "            ae.explainability_matrix = explainability\n",
    "    \n",
    "            \n",
    "            active_users = gm.get_users(short_train_g)\n",
    "            # e += f'\\nactive users : {user in active_users}'\n",
    "            #e += f'\\nNext Drug : D{drugs[i][1:]}'\n",
    "            \n",
    "            print(f'Drugs used for prediction :{drugs[:i]}')\n",
    "            print(f'\\nNext Drug to be predicted: {drugs[i]}')\n",
    "            #print(f'\\nRecommended Drugs list : [{recommendation}]')\n",
    "            for method in methods_to_be_evaluated:\n",
    "                rec_counter = 0\n",
    "                ae.evaluate_recommendation(rec=method[0], truth=drugs[i], method=method[1], s=s)\n",
    "                e += f'\\n{method[1]}_rec: ['\n",
    "                for rec in method[0]:\n",
    "                    rec_counter += 1\n",
    "                    if rec == drugs[i]:\n",
    "                        e += f'**D{rec[1:]}**'\n",
    "                        n_recommendation[f'{tw_i}_correct'] += 1\n",
    "                    else:\n",
    "                        e += f'D{rec[1:]}'\n",
    "                    if rec_counter < len(method[0]):\n",
    "                        e += ', '\n",
    "                e += ']'\n",
    "            for method in methos_to_be_evaluated_explainable:\n",
    "                rec_counter = 0\n",
    "                ae.evaluate_recommendation(rec=method[0], truth=drugs[i], method=method[1], s=s)\n",
    "                e += f'\\n{method[1]}_rec: ['\n",
    "                for rec in method[0]:\n",
    "                    rec_counter += 1\n",
    "                    if rec == drugs[i]:\n",
    "                        e += f'**D{rec[1:]}**'\n",
    "                        n_recommendation[f'{tw_i}_correct'] += 1\n",
    "                    else:\n",
    "                        e += f'D{rec[1:]}'\n",
    "                    if method[1]=='PathCount_combined':\n",
    "                        e += ' explained by '\n",
    "                        for index in rec_ab_df.columns:\n",
    "                            if rec_ab_df[index][rec] > 0:\n",
    "                                e += f'{index}: {rec_ab_df[index][rec]} '\n",
    "                    if rec_counter < len(method[0]):\n",
    "                            e += ', '\n",
    "                e += '] '\n",
    "\n",
    "                rec_ab_df = method[2]\n",
    "            if print_output:\n",
    "                print(e)\n",
    "            if len(methos_to_be_evaluated_explainable)>0:\n",
    "                rec_ab_df['sum'] = rec_ab_df.sum(axis=1)\n",
    "                rec_ab_df = rec_ab_df.sort_values(by='sum', ascending=True)\n",
    "                rec_ab_df = rec_ab_df.drop('sum',axis=1)\n",
    "\n",
    "                ax = rec_ab_df.plot(kind = 'barh',stacked=True, title=f'{user}-{drugs[i]}')\n",
    "                chart_values = np.asarray(rec_ab_df.values)\n",
    "                for ix in ax.patches:\n",
    "                    if ix._width>0:\n",
    "                        ax.text(ix.get_x() + (ix.get_width()/2), ix.get_y() + .2,str(round((ix.get_width()), 2)) , fontsize=15, color='black')\n",
    "                plt.show()\n",
    "    \n",
    "        ae.evaluate_session()\n",
    "\n",
    "    \n",
    "    ae.evaluate_tw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Graph Size for the remainder of notebook\n",
    "fig_size = (18,10)\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae.evaluate_total_performance()\n",
    "avg_n_ses_per_train_per_period = [round(np.mean(l)) for l in train_len_dict.values()]\n",
    "avg_ses_len_per_period = [round(np.mean(l), 2) for l in avg_ses_len.values()]\n",
    "e=\"\"\n",
    "# e +=(f'\\n\\n\\nNumber of sessions per user per short train period:\\n{sessions_per_user_in_short_term}')\n",
    "e +=(f'\\nNumber of recommendations per time split:{n_recommendation.values()}')\n",
    "e +=(f'\\nTotal # of recs:{sum(n_recommendation.values())}')\n",
    "e +=(f'\\nAverage # sessions per train per period {avg_n_ses_per_train_per_period}')\n",
    "e +=(f'\\nAverage # movies per session per period {avg_ses_len_per_period}')\n",
    "e +=(f'\\nAverage # sessions in train:{round(np.mean(train_set_len), 2)}')\n",
    "e +=(f'\\nAverage # movies in train:{round(np.mean(n_articles_train), 2)}')\n",
    "\n",
    "e+=('\\n---------- METHODS EVALUATION -------------')\n",
    "\n",
    "methods = [k for k, v in sorted(ae.precision.items(), key=itemgetter(1), reverse=True)]\n",
    "for m in methods:\n",
    "    e +=(f'\\n--- {m}: Precision:{ae.precision[m]}, NDCG:{ae.ndcg[m]},Explainability:{ae.explainability[m]}, ILD:{ae.diversity[m]}')#, ILD:{ae.diversity[m]},Explainability:{ae.explainability[m]}')\n",
    "print(e)\n",
    "p_start = tas.time_span_list[1][0]\n",
    "p_end = tas.time_span_list[len(tas.time_span_list) - 1][1] + datetime.timedelta(days=1)\n",
    "month_range = pd.date_range(p_start, p_end, freq='M')\n",
    "p = []\n",
    "# p.append(str(tas.time_window_graph_list[0][2]))\n",
    "empty_list =[]\n",
    "for time_window in tas.time_window_graph_list:\n",
    "            \n",
    "            if len(time_window[1])>0:\n",
    "                p.append((datetime.datetime.strftime(time_window[2], format='%Y-%m-%d'),datetime.datetime.strftime(time_window[3], format='%Y-%m-%d')))\n",
    "            else:\n",
    "                empty_list.append((datetime.datetime.strftime(time_window[2], format='%Y-%m-%d'),datetime.datetime.strftime(time_window[3], format='%Y-%m-%d')))\n",
    "# p = p[1:]\n",
    "traces =[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plot_df = pd.DataFrame(ae.tw_precision,index= p)\n",
    "plot_df = plot_df.loc[:, (plot_df != 0).any(axis=0)]\n",
    "# Setting the positions and width for the bars\n",
    "pos = list(range(plot_df.shape[0]))\n",
    "width =1/(plot_df.shape[1]+1)\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "colors = cm.rainbow(np.linspace(0,1,len(plot_df.columns)))\n",
    "width_mult = 0\n",
    "for column in plot_df.columns:\n",
    "    plt.bar([p + (width*width_mult) for p in pos],\n",
    "        #using df['pre_score'] data,\n",
    "        plot_df[column],\n",
    "        # of width\n",
    "        width,\n",
    "        # with alpha 0.5\n",
    "        alpha=0.5,\n",
    "        # with color\n",
    "        color=colors[width_mult],\n",
    "        # with label the first value in first_name\n",
    "        #label=plot_df['first_name'][0])\n",
    "        label = column\n",
    "            )\n",
    "    width_mult+=1\n",
    "# trace = go.Scatter(x = p,y=ae.tw_precision[method],name=method)\n",
    "#             traces.append(trace)\n",
    "ax.set_xticks([p + 2.5 * width for p in pos])\n",
    "\n",
    "# Set the labels for the x ticks\n",
    "plt.xlim(min(pos)-width, max(pos)+(plot_df.shape[1]+1)*width)\n",
    "plt.ylim([0, 1.5 ])\n",
    "ax.set_xticklabels(plot_df.index)\n",
    "plt.legend(plot_df.columns, loc='upper left')\n",
    "# for method in methods:\n",
    "#     if len(p)>len(ae.tw_precision[method]):\n",
    "#         ae.tw_precision[method] = [0] + ae.tw_precision[method] # We add a zero because the first value is always zero(the beginning)\n",
    "#     if np.asarray(ae.tw_precision[method]).sum()>0:\n",
    "#         plt.plot(p, ae.tw_precision[method], label=method)\n",
    "# trace = go.Scatter(x = p,y=ae.tw_precision[method],name=method)\n",
    "#             traces.append(trace)\n",
    "print(\"Current size:\", plt.rcParams[\"figure.figsize\"])\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Precision')\n",
    "ax.tick_params(axis='x', rotation=70)\n",
    "print(f'For the below time sessions, there where not enough data {empty_list}')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compnet",
   "language": "python",
   "name": "compnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
